{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "#tuning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score,GridSearchCV\n",
    "\n",
    "#models\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#evaluation\n",
    "from sklearn.metrics import f1_score,classification_report, roc_auc_score, confusion_matrix, accuracy_score, precision_score, recall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\n\u001b[1;32m      2\u001b[0m tensorflow\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39msave_model(drinking_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_drinking.pbtxt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m tensorflow\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39msave_model(smoking_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_drinking.pbtxt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "tensorflow.keras.models.save_model(drinking_model, 'model_drinking.pbtxt')\n",
    "tensorflow.keras.models.save_model(smoking_model, 'model_drinking.pbtxt')\n",
    "\n",
    "converterdrinking = tensorflow.lite.TFLiteConverter.from_keras_model(model = drinking_model)\n",
    "convertersmoking = tensorflow.lite.TFLiteConverter.from_keras_model(model = drinking_model)\n",
    "\n",
    "modeldrinking_tflite = converterdrinking.convert()\n",
    "modelsmoking_tflite = convertersmoking.convert()\n",
    "\n",
    "open(\"healthsignaldrinkingmodel.tflite\", \"wb\").write(modeldrinking_tflite)\n",
    "open(\"healthsignalsmokingmodel.tflite\", \"wb\").write(modelsmoking_tflite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Marshalikorawung/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.15.0\n",
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 08:56:21.640947: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: HTTP response code 302\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /Users/Marshalikorawung/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 100%|██████████| 5/5 [00:04<00:00,  1.09 file/s]\n",
      "2023-12-11 08:56:34.005044: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2023-12-11 08:56:34.005102: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2023-12-11 08:56:34.005121: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2023-12-11 08:56:34.005574: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-12-11 08:56:34.005963: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to /Users/Marshalikorawung/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n",
      "WARNING:tensorflow:AutoGraph could not transform <function normalize_img at 0x1065c7820> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function normalize_img at 0x1065c7820>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function normalize_img at 0x1065c7820> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function normalize_img at 0x1065c7820>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function normalize_img at 0x1065c7820> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function normalize_img at 0x1065c7820>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 08:56:35.921656: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-12-11 08:56:35.963590: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Adam/AssignAddVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 16s 21ms/step - loss: 0.1710 - accuracy: 0.9481 - val_loss: 0.0580 - val_accuracy: 0.9824\n",
      "Epoch 2/12\n",
      "469/469 [==============================] - 9s 18ms/step - loss: 0.0584 - accuracy: 0.9828 - val_loss: 0.0533 - val_accuracy: 0.9821\n",
      "Epoch 3/12\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.0507 - accuracy: 0.9855 - val_loss: 0.0478 - val_accuracy: 0.9863\n",
      "Epoch 4/12\n",
      "469/469 [==============================] - 9s 20ms/step - loss: 0.0595 - accuracy: 0.9853 - val_loss: 0.0916 - val_accuracy: 0.9818\n",
      "Epoch 5/12\n",
      "469/469 [==============================] - 9s 19ms/step - loss: 0.1005 - accuracy: 0.9843 - val_loss: 0.2259 - val_accuracy: 0.9778\n",
      "Epoch 6/12\n",
      "175/469 [==========>...................] - ETA: 4s - loss: 0.1644 - accuracy: 0.9842"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.config.list_physical_devices('GPU')\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "batch_size = 128\n",
    "ds_train = ds_train.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_train = ds_train.cache()\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(batch_size)\n",
    "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.map(\n",
    "    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.batch(batch_size)\n",
    "ds_test = ds_test.cache()\n",
    "ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu'),\n",
    "  tf.keras.layers.Conv2D(64, kernel_size=(3, 3),\n",
    "                 activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "#   tf.keras.layers.Dropout(0.25),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "#   tf.keras.layers.Dropout(0.5),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "model.fit(\n",
    "    ds_train,\n",
    "    epochs=12,\n",
    "    validation_data=ds_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Marshalikorawung/Library/Python/3.9/lib/python/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.api._v2.keras' has no attribute 'wrappers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m drinking_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Convert the scikit-learn model to a Keras model\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m keras_model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrappers\u001b[49m\u001b[38;5;241m.\u001b[39mscikit_learn\u001b[38;5;241m.\u001b[39mKerasClassifier(\n\u001b[1;32m     44\u001b[0m     build_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m: tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m     45\u001b[0m         tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],)),\n\u001b[1;32m     46\u001b[0m         tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m64\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     47\u001b[0m         tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m3\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     48\u001b[0m     ]),\n\u001b[1;32m     49\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Fit the Keras model with the scikit-learn model's predictions\u001b[39;00m\n\u001b[1;32m     53\u001b[0m keras_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'keras.api._v2.keras' has no attribute 'wrappers'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load Iris dataset for example\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create individual scikit-learn models\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "random_forest = RandomForestClassifier()\n",
    "knn = KNeighborsClassifier()\n",
    "neural_network = MLPClassifier()\n",
    "svm = SVC(probability=True)\n",
    "\n",
    "classification_models = {\n",
    "    'DecisionTreeClassifier': decision_tree,\n",
    "    'RandomForestClassifier': random_forest,\n",
    "    'KNeighborsClassifier': knn,\n",
    "    'NeuralNetworkClassifier': neural_network,\n",
    "    'SupportVectorMachine': svm\n",
    "}\n",
    "\n",
    "# Create a Voting Classifier with the individual models\n",
    "drinking_model = VotingClassifier(estimators=[\n",
    "    ('DecisionTreeClassifier', classification_models['DecisionTreeClassifier']),\n",
    "    ('RandomForestClassifier', classification_models['RandomForestClassifier']),\n",
    "    ('KNeighborsClassifier', classification_models['KNeighborsClassifier']),\n",
    "    ('NeuralNetworkClassifier', classification_models['NeuralNetworkClassifier']),\n",
    "    ('SupportVectorMachine', classification_models['SupportVectorMachine']),\n",
    "], voting=\"soft\")\n",
    "\n",
    "# Train the model\n",
    "drinking_model.fit(X_train, y_train)\n",
    "\n",
    "# Convert the scikit-learn model to a Keras model\n",
    "keras_model = tf.keras.wrappers.scikit_learn.KerasClassifier(\n",
    "    build_fn=lambda: tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(3, activation='softmax')\n",
    "    ]),\n",
    "    epochs=10\n",
    ")\n",
    "\n",
    "# Fit the Keras model with the scikit-learn model's predictions\n",
    "keras_model.fit(X_train, y_train)\n",
    "\n",
    "# Convert Keras model to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model.model)\n",
    "model_tflite = converter.convert()\n",
    "\n",
    "# Save TensorFlow Lite model\n",
    "open(\"healthsignal_drinking_model.tflite\", \"wb\").write(model_tflite)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
